/net/tscratch/people/plgjuliaryb
On Athena
/net/tscratch/people/plgjuliaryb/computational-linguistics-data
Flash attention successfully imported

=== Running technique: fp32 ===
Number of training sequences: 28049
Number of validation sequences: 24965
Searching for max batch size...
Max batch size: 496
TRAIN: USE_BF16 = False
  > Train Epoch Time: 16.04s
fp32 done.
Mean step time: 0.2554s
Peak memory: 33761.1 MB
Val perplexity: 12277184.90

=== Running technique: bf16 ===
Number of training sequences: 28049
Number of validation sequences: 24965
Using baseline batch size: 496
TRAIN: USE_BF16 = True
  > Train Epoch Time: 9.19s
bf16 done.
Mean step time: 0.1367s
Peak memory: 23086.5 MB
Val perplexity: 12207525.59

=== Running technique: bf16 ===
Number of training sequences: 28049
Number of validation sequences: 24965
Searching for max batch size...
Max batch size: 512
TRAIN: USE_BF16 = True
  > Train Epoch Time: 10.01s
bf16 done.
Mean step time: 0.1343s
Peak memory: 23828.6 MB
Val perplexity: 9053137.33

=== Running technique: fa2 ===
Number of training sequences: 28049
Number of validation sequences: 24965
Using baseline batch size: 496
TRAIN: USE_BF16 = True
  > Train Epoch Time: 8.77s
fa2 done.
Mean step time: 0.0980s
Peak memory: 20979.6 MB
Val perplexity: 15326332.84

=== Running technique: fa2 ===
Number of training sequences: 28049
Number of validation sequences: 24965
Searching for max batch size...
Max batch size: 896
TRAIN: USE_BF16 = True
  > Train Epoch Time: 7.39s
fa2 done.
Mean step time: 0.1654s
Peak memory: 37852.0 MB
Val perplexity: 497115787.62

=== Running technique: fa2_win128 ===
Number of training sequences: 28049
Number of validation sequences: 24965
Using baseline batch size: 496
TRAIN: USE_BF16 = True
  > Train Epoch Time: 7.23s
fa2_win128 done.
Mean step time: 0.1064s
Peak memory: 20977.2 MB
Val perplexity: 11151411.98

=== Running technique: fa2_win128 ===
Number of training sequences: 28049
Number of validation sequences: 24965
Searching for max batch size...
Max batch size: 896
TRAIN: USE_BF16 = True
  > Train Epoch Time: 7.06s
fa2_win128 done.
Mean step time: 0.1683s
Peak memory: 37852.0 MB
Val perplexity: 443161056.21

=== Running technique: fa2_win256 ===
Number of training sequences: 28049
Number of validation sequences: 24965
Using baseline batch size: 496
TRAIN: USE_BF16 = True
  > Train Epoch Time: 7.10s
fa2_win256 done.
Mean step time: 0.0957s
Peak memory: 20977.2 MB
Val perplexity: 11151411.98

=== Running technique: fa2_win256 ===
Number of training sequences: 28049
Number of validation sequences: 24965
Searching for max batch size...
Max batch size: 896
TRAIN: USE_BF16 = True
  > Train Epoch Time: 6.98s
fa2_win256 done.
Mean step time: 0.1666s
Peak memory: 37852.0 MB
Val perplexity: 443161056.21

=== Running technique: fp32_ckpt ===
Number of training sequences: 28049
Number of validation sequences: 24965
Using baseline batch size: 496
TRAIN: USE_BF16 = False
  > Train Epoch Time: 19.17s
fp32_ckpt done.
Mean step time: 0.3333s
Peak memory: 23691.5 MB
Val perplexity: 12444013.97

=== Running technique: fp32_ckpt ===
Number of training sequences: 28049
Number of validation sequences: 24965
Searching for max batch size...
Max batch size: 512
TRAIN: USE_BF16 = False
  > Train Epoch Time: 19.25s
fp32_ckpt done.
Mean step time: 0.3175s
Peak memory: 24454.1 MB
Val perplexity: 11155990.39

=== Running technique: bf16_ckpt ===
Number of training sequences: 28049
Number of validation sequences: 24965
Using baseline batch size: 496
TRAIN: USE_BF16 = True
  > Train Epoch Time: 10.66s
bf16_ckpt done.
Mean step time: 0.1606s
Peak memory: 17852.4 MB
Val perplexity: 10094627.45

=== Running technique: bf16_ckpt ===
Number of training sequences: 28049
Number of validation sequences: 24965
Searching for max batch size...
Max batch size: 1024
TRAIN: USE_BF16 = True
  > Train Epoch Time: 10.82s
bf16_ckpt done.
Mean step time: 0.3222s
Peak memory: 36796.6 MB
Val perplexity: 2690008307.90
