#!/bin/bash
#SBATCH --job-name=lstm
#SBATCH -A plgar2025-gpu-a100
#SBATCH -p plgrid-gpu-a100
#SBATCH --gres=gpu:a100:1
#SBATCH -N 1
#SBATCH -n 1
#SBATCH --cpus-per-task=8
#SBATCH --mem=32G
#SBATCH -t 08:00:00
#SBATCH -o $SCRATCH/computational-linguistics-data/logs/%x-%j.out
#SBATCH -e $SCRATCH/computational-linguistics-data/logs/%x-%j.err

set -euo pipefail
module purge
module load CUDA/12.1.1

source /net/people/plgrid/plgjuliaryb/uni/computational-linguistics/.venv/bin/activate

echo "Job $SLURM_JOB_ID on nodes: $SLURM_NODELIST"
echo "CUDA_VISIBLE_DEVICES=${CUDA_VISIBLE_DEVICES:-unset}"
python - <<'PY'
import torch
print("Torch:", torch.__version__, "CUDA:", torch.version.cuda, "available:", torch.cuda.is_available())
if torch.cuda.is_available():
    print("GPU:", torch.cuda.get_device_name(0))
PY

python train.py