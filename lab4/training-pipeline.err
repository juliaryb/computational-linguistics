 binutils/2.38 loaded.
 bzip2/1.0.8 loaded.
 zlib/1.2.12 loaded.
 ncurses/6.3 loaded.
 libreadline/8.1.2 loaded.
 Tcl/8.6.12 loaded.
 SQLite/3.38.3 loaded.
 XZ/5.2.5 loaded.
 GMP/6.2.1 loaded.
 libffi/3.4.2 loaded.
 OpenSSL/1.1 loaded.
 Python/3.10.4 loaded.
 CUDA/12.1.1 loaded.
sentencepiece_trainer.cc(178) LOG(INFO) Running command: --input=/net/tscratch/people/plgjuliaryb/computational-linguistics-data/data/lektury_train.txt --model_prefix=/net/tscratch/people/plgjuliaryb/computational-linguistics-data/tokenizers/sentence-piece --model_type=unigram --vocab_size=8000 --character_coverage=1.0 --normalization_rule_name=nfkc --add_dummy_prefix=true --byte_fallback=false --unk_id=0 --bos_id=1 --eos_id=2 --pad_id=3 --input_sentence_size=2000000 --shuffle_input_sentence=true
sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : 
trainer_spec {
  input: /net/tscratch/people/plgjuliaryb/computational-linguistics-data/data/lektury_train.txt
  input_format: 
  model_prefix: /net/tscratch/people/plgjuliaryb/computational-linguistics-data/tokenizers/sentence-piece
  model_type: UNIGRAM
  vocab_size: 8000
  self_test_sample_size: 0
  character_coverage: 1
  input_sentence_size: 2000000
  shuffle_input_sentence: 1
  seed_sentencepiece_size: 1000000
  shrinking_factor: 0.75
  max_sentence_length: 4192
  num_threads: 16
  num_sub_iterations: 2
  max_sentencepiece_length: 16
  split_by_unicode_script: 1
  split_by_number: 1
  split_by_whitespace: 1
  split_digits: 0
  pretokenization_delimiter: 
  treat_whitespace_as_suffix: 0
  allow_whitespace_only_pieces: 0
  required_chars: 
  byte_fallback: 0
  vocabulary_output_piece_score: 1
  train_extremely_large_corpus: 0
  seed_sentencepieces_file: 
  hard_vocab_limit: 1
  use_all_vocab: 0
  unk_id: 0
  bos_id: 1
  eos_id: 2
  pad_id: 3
  unk_piece: <unk>
  bos_piece: <s>
  eos_piece: </s>
  pad_piece: <pad>
  unk_surface:  â‡ 
  enable_differential_privacy: 0
  differential_privacy_noise_level: 0
  differential_privacy_clipping_threshold: 0
}
normalizer_spec {
  name: nfkc
  add_dummy_prefix: 1
  remove_extra_whitespaces: 1
  escape_whitespaces: 1
  normalization_rule_tsv: 
}
denormalizer_spec {}
trainer_interface.cc(355) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.
trainer_interface.cc(186) LOG(INFO) Loading corpus: /net/tscratch/people/plgjuliaryb/computational-linguistics-data/data/lektury_train.txt
trainer_interface.cc(382) LOG(WARNING) Found too long line (4942 > 4192).
trainer_interface.cc(384) LOG(WARNING) Too long lines are skipped in the training.
trainer_interface.cc(385) LOG(WARNING) The maximum length can be changed with --max_sentence_length=<size> flag.
trainer_interface.cc(411) LOG(INFO) Loaded all 208608 sentences
trainer_interface.cc(418) LOG(INFO) Skipped 68 too long sentences.
trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <unk>
trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <s>
trainer_interface.cc(427) LOG(INFO) Adding meta_piece: </s>
trainer_interface.cc(427) LOG(INFO) Adding meta_piece: <pad>
trainer_interface.cc(432) LOG(INFO) Normalizing sentences...
trainer_interface.cc(541) LOG(INFO) all chars count=25150994
trainer_interface.cc(562) LOG(INFO) Alphabet size=199
trainer_interface.cc(563) LOG(INFO) Final character coverage=1
trainer_interface.cc(594) LOG(INFO) Done! preprocessed 208607 sentences.
unigram_model_trainer.cc(265) LOG(INFO) Making suffix array...
unigram_model_trainer.cc(269) LOG(INFO) Extracting frequent sub strings... node_num=12457955
unigram_model_trainer.cc(312) LOG(INFO) Initialized 710361 seed sentencepieces
trainer_interface.cc(600) LOG(INFO) Tokenizing input sentences with whitespace: 208607
trainer_interface.cc(611) LOG(INFO) Done! 465918
unigram_model_trainer.cc(602) LOG(INFO) Using 465918 sentences for EM training
unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=217587 obj=13.0262 num_tokens=878734 num_tokens/piece=4.03854
unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=190611 obj=10.3478 num_tokens=880891 num_tokens/piece=4.62141
unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=142944 obj=10.3836 num_tokens=934299 num_tokens/piece=6.53612
unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=142822 obj=10.3529 num_tokens=934346 num_tokens/piece=6.54203
unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=107116 obj=10.5162 num_tokens=1008633 num_tokens/piece=9.41627
unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=107108 obj=10.4776 num_tokens=1008654 num_tokens/piece=9.41717
unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=80331 obj=10.6843 num_tokens=1085227 num_tokens/piece=13.5094
unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=80331 obj=10.6398 num_tokens=1085221 num_tokens/piece=13.5094
unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=60248 obj=10.8773 num_tokens=1162195 num_tokens/piece=19.2902
unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=60248 obj=10.8285 num_tokens=1162272 num_tokens/piece=19.2915
unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=45186 obj=11.0974 num_tokens=1238314 num_tokens/piece=27.4048
unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=45186 obj=11.0441 num_tokens=1238381 num_tokens/piece=27.4063
unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=33889 obj=11.3416 num_tokens=1313108 num_tokens/piece=38.7473
unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=33889 obj=11.2837 num_tokens=1313128 num_tokens/piece=38.7479
unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=25416 obj=11.6092 num_tokens=1388622 num_tokens/piece=54.6357
unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=25416 obj=11.547 num_tokens=1388636 num_tokens/piece=54.6363
unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=19062 obj=11.8935 num_tokens=1464089 num_tokens/piece=76.8067
unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=19062 obj=11.8279 num_tokens=1464163 num_tokens/piece=76.8106
unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=14296 obj=12.1992 num_tokens=1540250 num_tokens/piece=107.74
unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=14296 obj=12.1287 num_tokens=1540366 num_tokens/piece=107.748
unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=10722 obj=12.5212 num_tokens=1617729 num_tokens/piece=150.879
unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=10722 obj=12.4476 num_tokens=1617867 num_tokens/piece=150.892
unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=0 size=8800 obj=12.7238 num_tokens=1669912 num_tokens/piece=189.763
unigram_model_trainer.cc(618) LOG(INFO) EM sub_iter=1 size=8800 obj=12.673 num_tokens=1670230 num_tokens/piece=189.799
trainer_interface.cc(689) LOG(INFO) Saving model: /net/tscratch/people/plgjuliaryb/computational-linguistics-data/tokenizers/sentence-piece.model
trainer_interface.cc(701) LOG(INFO) Saving vocabs: /net/tscratch/people/plgjuliaryb/computational-linguistics-data/tokenizers/sentence-piece.vocab
/net/tscratch/people/plgjuliaryb/venvs/comp-lingu/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
/net/tscratch/people/plgjuliaryb/venvs/comp-lingu/lib/python3.10/site-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.
  warnings.warn(
