{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65707b07",
   "metadata": {},
   "source": [
    "# Lab 4: Memory-Efficient Transformer Training Techniques\n",
    "\n",
    "The goal of this assignment was to compare several modern memory-optimization techniques (BF16 mixed precision, FlashAttention, windowed attention and gradient checkpointing) used during Transformer training. The compared metrics include: GPU memory usage, maximum batch size that fits into memory, training speed (time per step and total time for 1 epoch) and final model performance (perplexity after 1 epoch).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94732899",
   "metadata": {},
   "source": [
    "### Datasets and Tokenizer\n",
    "\n",
    "The datasets used were downloaded from Speakleash (only high-quality docs were used, resulting in ~25MB files each)\n",
    "- `wolne_lektury_corpus` - for training  \n",
    "- `1000_novels_corpus_CLARIN-PL` - for validation \n",
    "\n",
    "The SentencePiece tokenizer was used (trained on the training corpus). Vocabulary size: 12000.\n",
    "\n",
    "Sequence Length was set to 256, and whole datasets were used, resulting in:\n",
    "- Number of training sequences: 28049\n",
    "- Number of validation sequences: 24965\n",
    "\n",
    "The datasets and splits are kept identical across all techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a25f4f",
   "metadata": {},
   "source": [
    "### Model architecture\n",
    "The from-scratch decoder-only language model was used (from Lab 1).\n",
    "\n",
    "Model initialisation and hyperparameters display."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dca6fe98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformerDecoderOnly(\n",
       "  (embed): Embedding(12000, 128, padding_idx=3)\n",
       "  (posenc): PositionalEncoding(\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (blocks): ModuleList(\n",
       "    (0-3): 4 x DecoderBlock(\n",
       "      (self_attn): MultiheadAttention(\n",
       "        (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (ff): Sequential(\n",
       "        (0): Linear(in_features=128, out_features=1024, bias=True)\n",
       "        (1): GELU(approximate='none')\n",
       "        (2): Linear(in_features=1024, out_features=128, bias=True)\n",
       "      )\n",
       "      (ln2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (drop): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (ln_f): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "  (lm_head): Linear(in_features=128, out_features=12000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "repo_root = Path(\"..\").resolve()\n",
    "sys.path.insert(0, str(repo_root))\n",
    "\n",
    "\n",
    "import config\n",
    "from model import TransformerDecoderOnly\n",
    "\n",
    "model = TransformerDecoderOnly(\n",
    "    vocab_size=config.VOCAB_SIZE,\n",
    "    d_model=config.TX_D_MODEL,\n",
    "    n_layer=config.TX_N_LAYER,\n",
    "    n_head=config.TX_N_HEAD,\n",
    "    d_ff=config.TX_D_FF,\n",
    "    dropout=config.TX_DROPOUT,\n",
    "    pad_id=3,\n",
    ")\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3df02e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Parameters: 2,855,680\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total Parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e7dfc5",
   "metadata": {},
   "source": [
    "Training hyperparameters (the setup can be found in `config.py`):\n",
    "- Learning rate: 1e-3\n",
    "- Optimizer: Adam\n",
    "- Number of epochs: just 1 for profiling different training regimes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae4bc0d",
   "metadata": {},
   "source": [
    "### Hardware used\n",
    "\n",
    "- Experiments were conducted on the Athena HPC cluster at AGH University of Science and Technology. \n",
    "- The SLURM scheduller was used and the script `training.sh` was used to run the experiment setup from `run_baseline.py`.\n",
    "- All experiments were run on a single NVIDIA A100-SXM4 GPU with 40 GB HBM2 memory.\n",
    "- The training stack used PyTorch 2.5.1 with CUDA 12.1 and FlashAttention 2.8.3.\n",
    "- The GPU natively supports BF16 mixed-precision arithmetic.\n",
    "\n",
    "```\n",
    "=== Hardware summary ===\n",
    "\n",
    "GPU: NVIDIA A100-SXM4-40GB\n",
    "GPU memory (GB): 39.7\n",
    "PyTorch: 2.5.1+cu121\n",
    "CUDA (torch): 12.1\n",
    "BF16 supported: True\n",
    "\n",
    "Name: flash_attn\n",
    "Version: 2.8.3\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092d5117",
   "metadata": {},
   "source": [
    "# Experiment setup\n",
    "For each technique, two configurations were run: (1) using the same batch size as the FP32 baseline, and (2) using the maximum batch size that fits into GPU memory. Peak memory usage and average step time are measured over 20 training steps, while training time and perplexity are measured after one full epoch. All other variables are kept identical."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e364c38",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "551b6847",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "computational-linguistics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
