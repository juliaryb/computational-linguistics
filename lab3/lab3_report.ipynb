{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4631730c",
   "metadata": {},
   "source": [
    "# Importing the dataset and data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d40a76",
   "metadata": {},
   "source": [
    "The AG News dataset is used for this assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a37a65d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "770770dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 120000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 7600\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"ag_news\")\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cffdb602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 120000\n",
      "})\n",
      "Dataset({\n",
      "    features: ['text', 'label'],\n",
      "    num_rows: 7600\n",
      "})\n",
      "\n",
      "Display a few samples:\n",
      "\n",
      " {'text': 'Nortel Warns of Weaker Sales  OTTAWA (Reuters) - Retreating from an upbeat forecast it  made just last month, Nortel Networks Corp. &lt;A HREF=\"http://www.investor.reuters.com/FullQuote.aspx?ticker=NT.TO target=/stocks/quickinfo/fullquote\"&gt;NT.TO&lt;/A&gt; warned on  Thursday that third-quarter sales will fall below  second-quarter levels and that its 2004 revenue growth rate  will lag that of the telecoms-equipment sector as a whole.', 'label': 2}\n",
      "\n",
      " {'text': 'Nikkei Up Over 1 Percent at Midday The Nikkei stock average rose 1.02 percent by midsession on Monday as investors were encouraged to seek bargains by a recovery in US stocks after crude oil prices retreated from record highs.', 'label': 2}\n",
      "\n",
      " {'text': 'Malaysia #39;s Abdullah Says Islamic Nations Should Rebuild Iraq Malaysian Prime Minister Abdullah Ahmad Badawi said Islamic nations, including Malaysia, need to help rebuild Iraq, a civilization that was once a symbol of Islam #39;s greatness.', 'label': 0}\n",
      "\n",
      " {'text': 'Europeans May Outfox US on Options The group that sets US accounting rules decided yesterday to delay for six months its plan to force companies to treat stock option grants as immediate expenses.', 'label': 2}\n",
      "\n",
      " {'text': 'CHELSEA DUO PAY CREDIT TO PSG Chelsea striker Didier Drogba and coach Jose Mourinho are convinced that Paris St Germain can bounce back from their crushing home defeat to the London side.', 'label': 1}\n"
     ]
    }
   ],
   "source": [
    "train_full = ds[\"train\"]\n",
    "test_full  = ds[\"test\"]\n",
    "\n",
    "print(train_full)\n",
    "print(test_full)\n",
    "\n",
    "print(\"\\nDisplay a few samples:\")\n",
    "for i in range(5):\n",
    "    print(\"\\n\", train_full[np.random.randint(train_full.num_rows)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8b37543",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label names: ['World', 'Sports', 'Business', 'Sci/Tech']\n",
      "\n",
      "Train label counts:\n",
      "  0 - World: 30000\n",
      "  1 - Sports: 30000\n",
      "  2 - Business: 30000\n",
      "  3 - Sci/Tech: 30000\n",
      "\n",
      "Test label counts:\n",
      "  0 - World: 1900\n",
      "  1 - Sports: 1900\n",
      "  2 - Business: 1900\n",
      "  3 - Sci/Tech: 1900\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "label_names = train_full.features[\"label\"].names\n",
    "print(\"Label names:\", label_names)\n",
    "\n",
    "train_label_counts = Counter(train_full[\"label\"])\n",
    "test_label_counts  = Counter(test_full[\"label\"])\n",
    "\n",
    "print(\"\\nTrain label counts:\")\n",
    "for idx, name in enumerate(label_names):\n",
    "    print(f\"  {idx} - {name}: {train_label_counts[idx]}\")\n",
    "\n",
    "print(\"\\nTest label counts:\")\n",
    "for idx, name in enumerate(label_names):\n",
    "    print(f\"  {idx} - {name}: {test_label_counts[idx]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337929ec",
   "metadata": {},
   "source": [
    "- the classes are perfectly balanced in this dataset\n",
    "\n",
    "Let's split the data into train, validation and test sets using stratification to keep the class ratio (though it could just be random since the classes are well-balanced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a2d90c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sizes:\n",
      "  train: 108000\n",
      "  val:   12000\n",
      "  test:  7600\n"
     ]
    }
   ],
   "source": [
    "split = train_full.train_test_split(\n",
    "    test_size=0.1,\n",
    "    stratify_by_column=\"label\",\n",
    "    seed=99,\n",
    ")\n",
    "\n",
    "train_raw = split[\"train\"]\n",
    "val_raw   = split[\"test\"]\n",
    "test_raw  = test_full  # keep HF test as final test set\n",
    "\n",
    "print(\"Sizes:\")\n",
    "print(\"  train:\", len(train_raw))\n",
    "print(\"  val:  \", len(val_raw))\n",
    "print(\"  test: \", len(test_raw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb935925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train label counts: Counter({0: 27000, 1: 27000, 2: 27000, 3: 27000})\n",
      "val label counts:   Counter({3: 3000, 1: 3000, 0: 3000, 2: 3000})\n",
      "test label counts:  Counter({2: 1900, 3: 1900, 1: 1900, 0: 1900})\n"
     ]
    }
   ],
   "source": [
    "print(\"train label counts:\", Counter(train_raw[\"label\"]))\n",
    "print(\"val label counts:  \", Counter(val_raw[\"label\"]))\n",
    "print(\"test label counts: \", Counter(test_raw[\"label\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c68ac1fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "  N samples: 108000\n",
      "  min words: 8\n",
      "  max words: 177\n",
      "  mean words: 37.8\n",
      "  median words: 37.0\n",
      "Val:\n",
      "  N samples: 12000\n",
      "  min words: 8\n",
      "  max words: 151\n",
      "  mean words: 37.9\n",
      "  median words: 37.0\n",
      "Test:\n",
      "  N samples: 7600\n",
      "  min words: 11\n",
      "  max words: 137\n",
      "  mean words: 37.7\n",
      "  median words: 37.0\n"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "\n",
    "def text_length_stats(ds, name):\n",
    "    lengths = [len(example[\"text\"].split()) for example in ds]\n",
    "    print(f\"{name}:\")\n",
    "    print(\"  N samples:\", len(lengths))\n",
    "    print(\"  min words:\", min(lengths))\n",
    "    print(\"  max words:\", max(lengths))\n",
    "    print(\"  mean words:\", round(statistics.mean(lengths), 1))\n",
    "    print(\"  median words:\", statistics.median(lengths))\n",
    "\n",
    "text_length_stats(train_raw, \"Train\")\n",
    "text_length_stats(val_raw,   \"Val\")\n",
    "text_length_stats(test_raw,  \"Test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c2e0503",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "lengths_train = [len(example[\"text\"].split()) for example in train_raw]\n",
    "plt.hist(lengths_train, bins=50)\n",
    "plt.xlabel(\"Text length (words)\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Distribution of article lengths in AG News (train split)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62b8607d",
   "metadata": {},
   "source": [
    "# Dataset Tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee477711",
   "metadata": {},
   "source": [
    "- most text lengths are around 38 words - and the majority fall under 50 words\n",
    "- since available tokenizers (like GPT-2’s BPE tokenizer) typically need around 1.5 tokens / word (just an estimate)\n",
    "    - 1.5 * 50 = 75 tokens\n",
    "- I will assume the sequence of 128 to be sufficient for each sample  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b01e142f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 50257\n",
      "Pad token id: 50256\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2TokenizerFast\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "tokenizer.pad_token = tokenizer.eos_token   # GPT-2 has no pad token by default and we need it for masking the input samples - shorter ones get padded so that all have a fixed length\n",
    "MAX_LEN = 128\n",
    "\n",
    "print(\"Vocab size:\", tokenizer.vocab_size)\n",
    "print(\"Pad token id:\", tokenizer.pad_token_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "beb64317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shapes:\n",
      "  input_ids:       torch.Size([16, 128])\n",
      "  attention_mask:  torch.Size([16, 128])\n",
      "  labels:          torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "# tokenize splits\n",
    "def tokenize_batch(batch):\n",
    "    return tokenizer(\n",
    "        batch[\"text\"],          # a LIST of raw text strings\n",
    "        truncation=True,        # cut off sequences longer than max_length\n",
    "        padding=\"max_length\",   # pad all sequences in batch to length MAX_LEN\n",
    "        max_length=MAX_LEN,     # the chosen sequence length\n",
    "    )\n",
    "\n",
    "train_tok = train_raw.map(tokenize_batch, batched=True)\n",
    "val_tok   = val_raw.map(tokenize_batch, batched=True)\n",
    "test_tok  = test_raw.map(tokenize_batch, batched=True)\n",
    "\n",
    "cols = [\"input_ids\", \"attention_mask\", \"label\"]\n",
    "for ds_tok in (train_tok, val_tok, test_tok):\n",
    "    ds_tok.set_format(type=\"torch\", columns=cols)\n",
    "\n",
    "# dataloaders for from-scratch model\n",
    "train_loader = DataLoader(train_tok, batch_size=16, shuffle=True)\n",
    "val_loader   = DataLoader(val_tok, batch_size=64, shuffle=False)\n",
    "test_loader  = DataLoader(test_tok, batch_size=64, shuffle=False)\n",
    "\n",
    "batch = next(iter(train_loader))\n",
    "print(\"Batch shapes:\")\n",
    "print(\"  input_ids:      \", batch[\"input_ids\"].shape)      # (B, 128)\n",
    "print(\"  attention_mask: \", batch[\"attention_mask\"].shape) # (B, 128)\n",
    "print(\"  labels:         \", batch[\"label\"].shape)          # (B,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d763b20",
   "metadata": {},
   "source": [
    "# Fine-tuning GPT-2 as a classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "675302e3",
   "metadata": {},
   "source": [
    "- At first \"gpt2\" was used but for cpu and unrestricted training dataset it yealded 40h worth of training for one epoch on cpu\n",
    "- it was therefore changed to \"distilgpt2\" since it has less parameters\n",
    "- the training dataset used was also restricted to a subset of 5k (can be increased if on cpu)\n",
    "- since freezing all transformer weights allowed for quicker training but with a significantly lower classification accuracy, I stuck to training without the weights frozen - slightly longer per epoch but starting at a significantly  higher classification accuracy, requiring less epochs to train to obtain good results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a66a5c1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels: ['World', 'Sports', 'Business', 'Sci/Tech']\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import evaluate\n",
    "from transformers import (\n",
    "    GPT2ForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    EarlyStoppingCallback,\n",
    ")\n",
    "\n",
    "# model config\n",
    "model_name = \"distilgpt2\"   # smaller, faster than full gpt2\n",
    "# model_name = \"gpt2\"\n",
    "\n",
    "num_labels = len(train_full.features[\"label\"].names)\n",
    "label_names = train_full.features[\"label\"].names\n",
    "print(\"Labels:\", label_names)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# metrics calculations - accuracy + macro F1\n",
    "accuracy_metric = evaluate.load(\"accuracy\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    acc = accuracy_metric.compute(predictions=preds, references=labels)[\"accuracy\"]\n",
    "    f1_macro = f1_metric.compute(\n",
    "        predictions=preds,\n",
    "        references=labels,\n",
    "        average=\"macro\"\n",
    "    )[\"f1\"]\n",
    "    return {\"accuracy\": acc, \"f1_macro\": f1_macro}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4dfe70a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at distilgpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params:      81915648\n",
      "Trainable params:  81915648\n"
     ]
    }
   ],
   "source": [
    "# load pre-trained GPT-2 for sequence classification\n",
    "model_ft = GPT2ForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels,\n",
    ")\n",
    "\n",
    "# use the same pad token as in the tokenizer\n",
    "model_ft.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "model_ft.to(device)\n",
    "\n",
    "# count trainable vs total parameters\n",
    "total_params = sum(p.numel() for p in model_ft.parameters())\n",
    "trainable_params = sum(p.numel() for p in model_ft.parameters() if p.requires_grad)\n",
    "print(f\"Total params:      {total_params}\")\n",
    "print(f\"Trainable params:  {trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f540754d",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"checkpoints/gpt2_full\",\n",
    "    eval_strategy=\"epoch\",       # eval at end of each epoch\n",
    "    save_strategy=\"epoch\",             # save checkpoint each epoch\n",
    "    learning_rate=5e-5,                # smaller LR for fine-tuning\n",
    "    per_device_train_batch_size=4,     # small per-device batch\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=4,     # effective train batch size ≈ 4*4 = 16\n",
    "    num_train_epochs=5,                # we will likely stop earlier via ES\n",
    "    weight_decay=0.01,                 # L2 regularization\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    logging_steps=100,\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\",                  # disable wandb etc.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "927d4df6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train subset size: 5000\n"
     ]
    }
   ],
   "source": [
    "# set a smaller training subset to keep runtime reasonable\n",
    "N_TRAIN = 5000\n",
    "train_tok_small = train_tok.select(range(N_TRAIN))\n",
    "print(\"Train subset size:\", len(train_tok_small))\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model_ft,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tok_small,\n",
    "    eval_dataset=val_tok,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ed12481",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julia/Documents/lingwistyka/computational-linguistics/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='939' max='1565' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 939/1565 49:20 < 32:57, 0.32 it/s, Epoch 3/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.325800</td>\n",
       "      <td>0.277631</td>\n",
       "      <td>0.901083</td>\n",
       "      <td>0.900596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.226900</td>\n",
       "      <td>0.295251</td>\n",
       "      <td>0.915083</td>\n",
       "      <td>0.915047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.182300</td>\n",
       "      <td>0.351888</td>\n",
       "      <td>0.914583</td>\n",
       "      <td>0.914404</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julia/Documents/lingwistyka/computational-linguistics/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/julia/Documents/lingwistyka/computational-linguistics/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning training time (s): 2962.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julia/Documents/lingwistyka/computational-linguistics/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='950' max='950' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [950/950 03:50]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned test metrics: {'eval_loss': 0.29934319853782654, 'eval_accuracy': 0.8948684210526315, 'eval_f1_macro': 0.8943923699743266, 'eval_runtime': 231.1813, 'eval_samples_per_second': 32.875, 'eval_steps_per_second': 4.109, 'epoch': 3.0}\n",
      "Inference time on test (s): 231.2\n"
     ]
    }
   ],
   "source": [
    "# train and measure training time\n",
    "start_train = time.time()\n",
    "train_result = trainer.train()\n",
    "end_train = time.time()\n",
    "ft_train_time = end_train - start_train\n",
    "print(f\"Fine-tuning training time (s): {ft_train_time:.1f}\")\n",
    "\n",
    "# evaluate on test set and measure inference time\n",
    "start_eval = time.time()\n",
    "test_metrics = trainer.evaluate(test_tok)\n",
    "end_eval = time.time()\n",
    "ft_inference_time = end_eval - start_eval\n",
    "\n",
    "print(\"Fine-tuned test metrics:\", test_metrics)\n",
    "print(f\"Inference time on test (s): {ft_inference_time:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "34e374a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved training log to lab3_logs/gpt2_full_log_history.csv\n",
      "Epochs run: [1.0, 2.0, 3.0]\n",
      "Saved summary to lab3_logs/gpt2_full_summary.json\n"
     ]
    }
   ],
   "source": [
    "import os, json\n",
    "import pandas as pd\n",
    "\n",
    "os.makedirs(\"lab3_logs\", exist_ok=True)\n",
    "\n",
    "# 1) Save HF Trainer log history for curves\n",
    "log_history = trainer.state.log_history\n",
    "log_df = pd.DataFrame(log_history)\n",
    "log_df.to_csv(\"lab3_logs/gpt2_full_log_history.csv\", index=False)\n",
    "print(\"Saved training log to lab3_logs/gpt2_full_log_history.csv\")\n",
    "\n",
    "# 2) Compute how many epochs actually ran (use eval entries)\n",
    "epochs_run = sorted({entry[\"epoch\"] for entry in log_history if \"eval_loss\" in entry})\n",
    "num_epochs_run = len(epochs_run)\n",
    "print(\"Epochs run:\", epochs_run)\n",
    "\n",
    "# 3) Build a summary dict for this run\n",
    "summary = {\n",
    "    \"model_name\": model_name,\n",
    "    \"train_subset_size\": N_TRAIN,\n",
    "    \"num_epochs_requested\": training_args.num_train_epochs,\n",
    "    \"num_epochs_run\": num_epochs_run,\n",
    "    \"total_train_time_sec\": ft_train_time,\n",
    "    \"train_time_per_epoch_sec\": ft_train_time / max(num_epochs_run, 1),\n",
    "    \"inference_time_sec\": ft_inference_time,\n",
    "    \"total_params\": int(total_params),\n",
    "    \"trainable_params\": int(trainable_params),\n",
    "    \"test_metrics\": test_metrics,  # contains accuracy, f1, eval_loss, etc.\n",
    "}\n",
    "\n",
    "with open(\"lab3_logs/gpt2_full_summary.json\", \"w\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"Saved summary to lab3_logs/gpt2_full_summary.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bb2420",
   "metadata": {},
   "source": [
    "# Weight-frozen training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a670444",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at distilgpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total params:      81915648\n",
      "Trainable params:  3072\n"
     ]
    }
   ],
   "source": [
    "model_ft_f = GPT2ForSequenceClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=num_labels,\n",
    ")\n",
    "\n",
    "model_ft_f.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# freeze transformer, train only classification head\n",
    "# freeze all transformer layers\n",
    "for param in model_ft_f.transformer.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# ensure classification head is trainable\n",
    "for param in model_ft_f.score.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "model_ft_f.to(device)\n",
    "\n",
    "# count trainable vs total parameters\n",
    "total_params = sum(p.numel() for p in model_ft_f.parameters())\n",
    "trainable_params = sum(p.numel() for p in model_ft_f.parameters() if p.requires_grad)\n",
    "print(f\"Total params:      {total_params}\")\n",
    "print(f\"Trainable params:  {trainable_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "95f0de98",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"checkpoints/gpt2_frozen\",\n",
    "    eval_strategy=\"epoch\",       # eval at end of each epoch\n",
    "    save_strategy=\"epoch\",             # save checkpoint each epoch\n",
    "    learning_rate=5e-5,                # smaller LR for fine-tuning\n",
    "    per_device_train_batch_size=4,     # small per-device batch\n",
    "    per_device_eval_batch_size=8,\n",
    "    gradient_accumulation_steps=4,     # effective train batch size ≈ 4*4 = 16\n",
    "    num_train_epochs=5,                # we will likely stop earlier via ES\n",
    "    weight_decay=0.01,                 # L2 regularization\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    logging_steps=100,\n",
    "    save_total_limit=2,\n",
    "    report_to=\"none\",                  # disable wandb etc.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "496bd7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model_ft_f,\n",
    "    args=training_args,\n",
    "    train_dataset=train_tok_small,\n",
    "    eval_dataset=val_tok,\n",
    "    processing_class=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6877d1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julia/Documents/lingwistyka/computational-linguistics/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1565' max='1565' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1565/1565 47:39, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1 Macro</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.344300</td>\n",
       "      <td>1.160406</td>\n",
       "      <td>0.529417</td>\n",
       "      <td>0.526414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.986400</td>\n",
       "      <td>0.902601</td>\n",
       "      <td>0.708417</td>\n",
       "      <td>0.703015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.853300</td>\n",
       "      <td>0.791489</td>\n",
       "      <td>0.767583</td>\n",
       "      <td>0.765686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.812400</td>\n",
       "      <td>0.743937</td>\n",
       "      <td>0.787250</td>\n",
       "      <td>0.785741</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.779400</td>\n",
       "      <td>0.728588</td>\n",
       "      <td>0.794250</td>\n",
       "      <td>0.792991</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julia/Documents/lingwistyka/computational-linguistics/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/julia/Documents/lingwistyka/computational-linguistics/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/julia/Documents/lingwistyka/computational-linguistics/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/home/julia/Documents/lingwistyka/computational-linguistics/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning training time (s): 2860.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julia/Documents/lingwistyka/computational-linguistics/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='950' max='950' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [950/950 03:42]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned test metrics: {'eval_loss': 0.7349355220794678, 'eval_accuracy': 0.7893421052631578, 'eval_f1_macro': 0.7877586660061471, 'eval_runtime': 222.4879, 'eval_samples_per_second': 34.159, 'eval_steps_per_second': 4.27, 'epoch': 5.0}\n",
      "Inference time on test (s): 222.5\n"
     ]
    }
   ],
   "source": [
    "# train and measure training time\n",
    "start_train = time.time()\n",
    "train_result_f = trainer.train()\n",
    "end_train = time.time()\n",
    "ft_train_time_f = end_train - start_train\n",
    "print(f\"Fine-tuning training time (s): {ft_train_time_f:.1f}\")\n",
    "\n",
    "# evaluate on test set and measure inference time\n",
    "start_eval = time.time()\n",
    "test_metrics_f = trainer.evaluate(test_tok)\n",
    "end_eval = time.time()\n",
    "ft_inference_time_f = end_eval - start_eval\n",
    "\n",
    "print(\"Fine-tuned test metrics:\", test_metrics_f)\n",
    "print(f\"Inference time on test (s): {ft_inference_time_f:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "00607193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved training log to lab3_logs/gpt2_frozen_log_history.csv\n",
      "Epochs run: [1.0, 2.0, 3.0, 4.0, 5.0]\n",
      "Saved summary to lab3_logs/gpt2_frozen_summary.json\n"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"lab3_logs\", exist_ok=True)\n",
    "\n",
    "# 1) Save HF Trainer log history for curves\n",
    "log_history = trainer.state.log_history\n",
    "log_df = pd.DataFrame(log_history)\n",
    "log_df.to_csv(\"lab3_logs/gpt2_frozen_log_history.csv\", index=False)\n",
    "print(\"Saved training log to lab3_logs/gpt2_frozen_log_history.csv\")\n",
    "\n",
    "# 2) Compute how many epochs actually ran (use eval entries)\n",
    "epochs_run = sorted({entry[\"epoch\"] for entry in log_history if \"eval_loss\" in entry})\n",
    "num_epochs_run = len(epochs_run)\n",
    "print(\"Epochs run:\", epochs_run)\n",
    "\n",
    "# 3) Build a summary dict for this run\n",
    "summary = {\n",
    "    \"model_name\": model_name,\n",
    "    \"train_subset_size\": N_TRAIN,\n",
    "    \"num_epochs_requested\": training_args.num_train_epochs,\n",
    "    \"num_epochs_run\": num_epochs_run,\n",
    "    \"total_train_time_sec\": ft_train_time,\n",
    "    \"train_time_per_epoch_sec\": ft_train_time / max(num_epochs_run, 1),\n",
    "    \"inference_time_sec\": ft_inference_time,\n",
    "    \"total_params\": int(total_params),\n",
    "    \"trainable_params\": int(trainable_params),\n",
    "    \"test_metrics\": test_metrics,  # contains accuracy, f1, eval_loss, etc.\n",
    "}\n",
    "\n",
    "with open(\"lab3_logs/gpt2_frozen_summary.json\", \"w\") as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"Saved summary to lab3_logs/gpt2_frozen_summary.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb83204",
   "metadata": {},
   "source": [
    "# The from-scratch GPT-like model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fe74bade",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scratch train batch shapes:\n",
      "  input_ids:       torch.Size([16, 128])\n",
      "  attention_mask:  torch.Size([16, 128])\n",
      "  labels:          torch.Size([16])\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# For fair comparison, use the same subset as for fine-tuning\n",
    "# (if you used N_TRAIN before, reuse it; otherwise set it here)\n",
    "N_TRAIN = 5000\n",
    "train_tok_small = train_tok.select(range(N_TRAIN))\n",
    "\n",
    "batch_size_train = 16\n",
    "batch_size_eval = 64\n",
    "\n",
    "train_loader_scratch = DataLoader(train_tok_small, batch_size=batch_size_train, shuffle=True)\n",
    "val_loader_scratch   = DataLoader(val_tok,         batch_size=batch_size_eval,   shuffle=False)\n",
    "test_loader_scratch  = DataLoader(test_tok,        batch_size=batch_size_eval,   shuffle=False)\n",
    "\n",
    "batch = next(iter(train_loader_scratch))\n",
    "print(\"Scratch train batch shapes:\")\n",
    "print(\"  input_ids:      \", batch[\"input_ids\"].shape)\n",
    "print(\"  attention_mask: \", batch[\"attention_mask\"].shape)\n",
    "print(\"  labels:         \", batch[\"label\"].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e23e9e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TinyGPTClassifier(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        vocab_size: int,\n",
    "        num_labels: int,\n",
    "        max_len: int = 128,\n",
    "        d_model: int = 128,\n",
    "        n_heads: int = 4,\n",
    "        n_layers: int = 2,\n",
    "        dim_ff: int = 512,\n",
    "        dropout: float = 0.1,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_labels = num_labels\n",
    "        self.d_model = d_model\n",
    "        self.max_len = max_len\n",
    "\n",
    "        # Token and positional embeddings\n",
    "        self.token_emb = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_emb   = nn.Embedding(max_len, d_model)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=n_heads,\n",
    "            dim_feedforward=dim_ff,\n",
    "            dropout=dropout,\n",
    "            batch_first=True,   # [B, T, D]\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            encoder_layer,\n",
    "            num_layers=n_layers,\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Classification head: simple linear layer\n",
    "        self.classifier = nn.Linear(d_model, num_labels)\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        # Initialize weights reasonably\n",
    "        nn.init.normal_(self.token_emb.weight, mean=0.0, std=0.02)\n",
    "        nn.init.normal_(self.pos_emb.weight,   mean=0.0, std=0.02)\n",
    "        nn.init.normal_(self.classifier.weight, mean=0.0, std=0.02)\n",
    "        nn.init.zeros_(self.classifier.bias)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        input_ids:      [B, T]\n",
    "        attention_mask: [B, T] with 1 for real tokens, 0 for padding\n",
    "        \"\"\"\n",
    "        B, T = input_ids.shape\n",
    "        device = input_ids.device\n",
    "\n",
    "        # 1) Embeddings\n",
    "        pos_ids = torch.arange(T, device=device).unsqueeze(0).expand(B, T)  # [B, T]\n",
    "        x = self.token_emb(input_ids) + self.pos_emb(pos_ids)              # [B, T, D]\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # 2) Optional causal mask for decoder-style behavior\n",
    "        #    (upper triangular mask to prevent attending to future tokens)\n",
    "        #    For classification, this is not critical, but it makes it \"decoder-like\".\n",
    "        attn_mask = None\n",
    "\n",
    "        # 3) Apply transformer encoder (with batch_first=True)\n",
    "        #    src_key_padding_mask expects True for padding tokens\n",
    "        src_key_padding_mask = (attention_mask == 0)  # [B, T], True where padding\n",
    "        h = self.transformer(\n",
    "            x,\n",
    "            mask=attn_mask,\n",
    "            src_key_padding_mask=src_key_padding_mask,\n",
    "        )  # [B, T, D]\n",
    "\n",
    "        # 4) Mean pooling over non-padded tokens\n",
    "        mask = attention_mask.unsqueeze(-1)  # [B, T, 1]\n",
    "        h_masked = h * mask                  # zero out padding\n",
    "        sum_h = h_masked.sum(dim=1)         # [B, D]\n",
    "        lengths = mask.sum(dim=1).clamp(min=1)  # [B, 1]\n",
    "        pooled = sum_h / lengths            # [B, D]\n",
    "\n",
    "        pooled = self.dropout(pooled)\n",
    "\n",
    "        # 5) Classification head\n",
    "        logits = self.classifier(pooled)    # [B, num_labels]\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8df8e166",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "def train_scratch_model(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    num_epochs: int = 5,\n",
    "    base_lr: float = 3e-4,\n",
    "    weight_decay: float = 0.01,\n",
    "    max_grad_norm: float = 1.0,\n",
    "    warmup_ratio: float = 0.1,\n",
    "    device=None,\n",
    "):\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=base_lr,\n",
    "        weight_decay=weight_decay,\n",
    "    )\n",
    "\n",
    "    num_training_steps = num_epochs * len(train_loader)\n",
    "    num_warmup_steps = int(warmup_ratio * num_training_steps)\n",
    "\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer,\n",
    "        num_warmup_steps=num_warmup_steps,\n",
    "        num_training_steps=num_training_steps,\n",
    "    )\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    history = {\n",
    "        \"epoch\": [],\n",
    "        \"train_loss\": [],\n",
    "        \"val_loss\": [],\n",
    "        \"val_acc\": [],\n",
    "        \"val_f1\": [],\n",
    "    }\n",
    "\n",
    "    from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "    global_step = 0\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            logits = model(input_ids, attention_mask)   # [B, num_labels]\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            # Gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            global_step += 1\n",
    "\n",
    "            running_loss += loss.item() * input_ids.size(0)\n",
    "\n",
    "        train_loss_epoch = running_loss / len(train_loader.dataset)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        all_preds = []\n",
    "        all_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in val_loader:\n",
    "                input_ids = batch[\"input_ids\"].to(device)\n",
    "                attention_mask = batch[\"attention_mask\"].to(device)\n",
    "                labels = batch[\"label\"].to(device)\n",
    "\n",
    "                logits = model(input_ids, attention_mask)\n",
    "                loss = criterion(logits, labels)\n",
    "                val_losses.append(loss.item() * input_ids.size(0))\n",
    "\n",
    "                preds = torch.argmax(logits, dim=-1)\n",
    "                all_preds.extend(preds.cpu().tolist())\n",
    "                all_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "        val_loss_epoch = sum(val_losses) / len(val_loader.dataset)\n",
    "        val_acc = accuracy_score(all_labels, all_preds)\n",
    "        val_f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
    "\n",
    "        history[\"epoch\"].append(epoch)\n",
    "        history[\"train_loss\"].append(train_loss_epoch)\n",
    "        history[\"val_loss\"].append(val_loss_epoch)\n",
    "        history[\"val_acc\"].append(val_acc)\n",
    "        history[\"val_f1\"].append(val_f1)\n",
    "\n",
    "        print(\n",
    "            f\"Epoch {epoch:02d} | \"\n",
    "            f\"train_loss={train_loss_epoch:.4f} | \"\n",
    "            f\"val_loss={val_loss_epoch:.4f} | \"\n",
    "            f\"val_acc={val_acc:.4f} | \"\n",
    "            f\"val_f1={val_f1:.4f}\"\n",
    "        )\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "41c836a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/julia/Documents/lingwistyka/computational-linguistics/.venv/lib/python3.11/site-packages/torch/nn/modules/transformer.py:515: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. We recommend specifying layout=torch.jagged when constructing a nested tensor, as this layout receives active development, has better operator coverage, and works with torch.compile. (Triggered internally at /pytorch/aten/src/ATen/NestedTensorImpl.cpp:178.)\n",
      "  output = torch._nested_tensor_from_mask(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | train_loss=1.0231 | val_loss=0.6399 | val_acc=0.7505 | val_f1=0.7513\n",
      "Epoch 02 | train_loss=0.2844 | val_loss=0.4419 | val_acc=0.8562 | val_f1=0.8564\n",
      "Epoch 03 | train_loss=0.0613 | val_loss=0.5794 | val_acc=0.8622 | val_f1=0.8613\n",
      "Epoch 04 | train_loss=0.0093 | val_loss=0.6167 | val_acc=0.8695 | val_f1=0.8694\n",
      "Epoch 05 | train_loss=0.0023 | val_loss=0.6374 | val_acc=0.8691 | val_f1=0.8687\n"
     ]
    }
   ],
   "source": [
    "vocab_size = tokenizer.vocab_size\n",
    "num_labels = len(train_full.features[\"label\"].names)\n",
    "\n",
    "tiny_model = TinyGPTClassifier(\n",
    "    vocab_size=vocab_size,\n",
    "    num_labels=num_labels,\n",
    "    max_len=MAX_LEN,\n",
    "    d_model=128,\n",
    "    n_heads=4,\n",
    "    n_layers=2,\n",
    "    dim_ff=512,\n",
    "    dropout=0.1,\n",
    ")\n",
    "\n",
    "history_scratch = train_scratch_model(\n",
    "    tiny_model,\n",
    "    train_loader_scratch,\n",
    "    val_loader_scratch,\n",
    "    num_epochs=5,\n",
    "    base_lr=3e-4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3f652114",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scratch test metrics: {'accuracy': 0.8565789473684211, 'f1_macro': 0.8562688926288384, 'inference_time_s': 3.3778457641601562}\n",
      "Scratch model parameter count: 6846340\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "def eval_on_test(model, test_loader, device=None):\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    import time\n",
    "    start = time.time()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch[\"input_ids\"].to(device)\n",
    "            attention_mask = batch[\"attention_mask\"].to(device)\n",
    "            labels = batch[\"label\"].to(device)\n",
    "\n",
    "            logits = model(input_ids, attention_mask)\n",
    "            preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().tolist())\n",
    "            all_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "    end = time.time()\n",
    "    total_time = end - start\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1_macro = f1_score(all_labels, all_preds, average=\"macro\")\n",
    "\n",
    "    return {\"accuracy\": acc, \"f1_macro\": f1_macro, \"inference_time_s\": total_time}\n",
    "\n",
    "scratch_test_metrics = eval_on_test(tiny_model, test_loader_scratch)\n",
    "print(\"Scratch test metrics:\", scratch_test_metrics)\n",
    "\n",
    "scratch_param_count = sum(p.numel() for p in tiny_model.parameters())\n",
    "print(\"Scratch model parameter count:\", scratch_param_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c07cb07",
   "metadata": {},
   "source": [
    "### Minimal working setup - changing params and running overnight to get more meaningful results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f422bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "computational-linguistics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
